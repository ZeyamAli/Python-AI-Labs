{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBQjJKklKukh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "data = np.loadtxt(r'C:\\Users\\Muhammad Saad\\Desktop\\AI (Muhammad Saad)(BCS21258)\\Lab10\\ex1data1.txt', delimiter=',')\n",
        "\n",
        "\n",
        "# Split data into x_train (Population) and y_train (Profit)\n",
        "x_train = data[:, 0]\n",
        "y_train = data[:, 1]\n",
        "\n",
        "\n",
        "def compute_cost(x, y, w, b):\n",
        "    m = x.shape[0]  # number of training examples\n",
        "    total_cost = 0\n",
        "\n",
        "    for i in range(m):\n",
        "        # Prediction (f_wb)\n",
        "        f_wb = w * x[i] + b\n",
        "        # Square of the error\n",
        "        total_cost += (f_wb - y[i]) ** 2\n",
        "\n",
        "    # Cost calculation\n",
        "    return total_cost / (2 * m)\n",
        "\n",
        "def gradient_descent(x, y, w, b, learning_rate, num_iters):\n",
        "    m = len(x)  # number of training examples\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        # Initialize gradients\n",
        "        dw, db = 0, 0\n",
        "\n",
        "        for j in range(m):\n",
        "            f_wb = w * x[j] + b  # Predicted value\n",
        "            dw += (f_wb - y[j]) * x[j]  # Gradient w.r.t. w\n",
        "            db += (f_wb - y[j])  # Gradient w.r.t. b\n",
        "\n",
        "        # Update w and b\n",
        "        w -= learning_rate * dw / m\n",
        "        b -= learning_rate * db / m\n",
        "\n",
        "        # Print the cost every 100 iterations (optional)\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}: Cost = {compute_cost(x, y, w, b)}\")\n",
        "\n",
        "    return w, b\n",
        "\n",
        "# Initial values\n",
        "initial_w = 0\n",
        "initial_b = 0\n",
        "learning_rate = 0.01\n",
        "num_iters = 1000\n",
        "\n",
        "# Perform Gradient Descent\n",
        "w, b = gradient_descent(x_train, y_train, initial_w, initial_b, learning_rate, num_iters)\n",
        "print(f\"Optimized w: {w}, Optimized b: {b}\")\n",
        "\n",
        "# Predicted values\n",
        "predictions = w * x_train + b\n",
        "\n",
        "# Plotting\n",
        "plt.scatter(x_train, y_train, color=\"red\", label=\"Actual data\")\n",
        "plt.plot(x_train, predictions, label=\"Linear Regression\")\n",
        "\n",
        "# Annotate the slope (w) and intercept (b) on the graph\n",
        "plt.text(15, 20, f\"Slope (w): {w:.4f}\", fontsize=12, color=\"blue\")\n",
        "plt.text(15, 18, f\"Intercept (b): {b:.4f}\", fontsize=12, color=\"blue\")\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Population (in 10,000s)\")\n",
        "plt.ylabel(\"Profit (in $10,000s)\")\n",
        "plt.title(\"Population vs Profit\")\n",
        "\n",
        "# Legend\n",
        "plt.legend()\n",
        "\n",
        "# Display the graph\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}